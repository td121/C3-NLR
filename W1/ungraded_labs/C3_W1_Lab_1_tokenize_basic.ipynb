{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvAhGwrxjVjp"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W1/ungraded_labs/C3_W1_Lab_1_tokenize_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL-LzAqpoGLC"
      },
      "source": [
        "# Ungraded Lab: Tokenizer Basics\n",
        "\n",
        "In most NLP tasks, the initial step in preparing your data is to extract a vocabulary of words from your *corpus* (i.e. input texts). You will need to define how to represent the texts into numerical representations which can be used to train a neural network. These representations are called *tokens* and Tensorflow and Keras makes it easy to generate these using its APIs. You will see how to do that in the next cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nt3uR9TPrUt"
      },
      "source": [
        "## Generating the vocabulary\n",
        "\n",
        "In this notebook, you will look first at how you can provide a look up dictionary for each word. The code below takes a list of sentences, then takes each word in those sentences and assigns it to an integer. This is done using the [fit_on_texts()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts) method and you can get the result by looking at the `word_index` property. More frequent words have a lower index."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "Js93XWX4j_YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaCMcjMQifQc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Define input sentences\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat'\n",
        "    ]\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "\n",
        "# Generate indices for each word in the corpus\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Get the indices and print it\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTPWesNaRdX2"
      },
      "source": [
        "The `num_words` parameter used in the initializer specifies the maximum number of words minus one (based on frequency) to keep when generating sequences. You will see this in a later exercise. For now, the important thing to note is it does not affect how the `word_index` dictionary is generated. You can try passing `1` instead of `100` as shown on the next cell and you will arrive at the same `word_index`. \n",
        "\n",
        "Also notice that by default, all punctuation is ignored and words are converted to lower case. You can override these behaviors by modifying the `filters` and `lower` arguments of the `Tokenizer` class as described [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#arguments). You can try modifying these in the next cell below and compare the output to the one generated above."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply TextVectorization instead of using 'Tokenizer' "
      ],
      "metadata": {
        "id": "IHQFCvl0nHtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "# Define input sentences\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat'\n",
        "]\n",
        "\n",
        "# Create an instance of the TextVectorization layer\n",
        "vectorizer = TextVectorization(max_tokens=100, output_mode='int', output_sequence_length=None)\n",
        "\n",
        "# Adapt the TextVectorization layer to the input sentences\n",
        "vectorizer.adapt(sentences)\n",
        "\n",
        "# Get the vocabulary\n",
        "vocabulary = vectorizer.get_vocabulary()\n",
        "word_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "\n",
        "# Print the word_index\n",
        "print(word_index)\n"
      ],
      "metadata": {
        "id": "JwqCrJp3nP0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX1A1pDNoVKm"
      },
      "outputs": [],
      "source": [
        "# Define input sentences\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=100,\n",
        "    filters='',  \n",
        "    lower=False,\n",
        "    split=' ',          # biggest limitation of tokenizer is that not allowing mutiple spliter\n",
        "    char_level=False,\n",
        "    oov_token=None,\n",
        "    analyzer=None,\n",
        ")\n",
        "\n",
        "# Generate indices for each word in the corpus\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Get the indices and print it\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Tokenizer:\n",
        "\n",
        "Tokenizer is a class provided by the Keras API in TensorFlow for text preprocessing. It handles tokenization, building a vocabulary from the training data, converting text to sequences (integer representations), and optional padding. The main focus of Tokenizer is to create a mapping between words and their corresponding integer values. It is typically used in combination with the pad_sequences function for padding.\n",
        "\n",
        "2.   TextVectorization:\n",
        "\n",
        "TextVectorization is a layer provided by the TensorFlow Keras API that can be directly added to a neural network model. It performs similar functions to Tokenizer but can also be used for more advanced text preprocessing, such as lowercasing, removing punctuation, and splitting text by whitespace. TextVectorization can also be configured to output one-hot-encoded or count-based representations of text, in addition to integer sequences. The main advantage of using TextVectorization is that it integrates seamlessly with the model architecture and can be part of the model itself, making the model more portable and easier to deploy.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c1u5bSx7pVpp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9LFfwBffDaj"
      },
      "source": [
        "That concludes this short exercise on tokenizing input texts!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "C3_W1_Lab_1_tokenize_basic.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}